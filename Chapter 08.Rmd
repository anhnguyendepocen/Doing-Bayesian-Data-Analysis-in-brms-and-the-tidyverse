---
title: "Chapter 08. ~~JAGS~~ brms"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

```{r set-options, echo = FALSE, cachse = FALSE}
options(width = 100)
```

## 8.1. ~~JAGS~~ brms and its relation to R

In the opening prargraph in his Github repository for brms, Bürkner explains:

>The **brms** package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of distributions and link functions are supported, allowing users to fit -- among others -- linear, robust linear, count data, survival, response times, ordinal, zero-inflated, hurdle, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e. models with multiple response variables) can be fitted, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, leave-one-out cross-validation, and Bayes factors.

Bürkner's brms repository includes many helpful links, such as to where [brms lives on CRAN](https://cran.r-project.org/web/packages/brms/index.html), a [list of blog posts](https://paul-buerkner.github.io/blog/brms-blogposts/) highlighting brms, and [a forum](http://discourse.mc-stan.org) where users can ask questions about brms in specific or about Stan in general.

You can install the current official version of brms in the same way you would any other R package. If you want the current developmental version, you could downloaded it from github via:

```{r, eval = F}
if (!requireNamespace("devtools")) {
  install.packages("devtools")
}
devtools::install_github("paul-buerkner/brms")
```

## 8.2. A complete example

### 8.2.1. Load data.

Here we load the data.

```{r, warning = F, message = F}
library(tidyverse)

z15N50 <- read_csv("data.R/z15N50.csv")
```

The brms package does not require us to convert the data into a list. It can handle data in lists or data frames, of which [tibbles are a special case](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html). Here are what the data look like.

```{r}
head(z15N50)
```

Here they are in a bar plot.

```{r, echo = F}
theme_set(theme_grey())
```

```{r, fig.width = 3, fig.height = 3}
z15N50 %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(x = y)) +
  geom_bar() +
  theme(panel.grid = element_blank())
```

### 8.2.2. Specify model.

The brms package doesn't have code blocks following the JAGS format or the sequence in Kurschke's diagrams. Rather, its syntax is modeled in part after the popular frequentist mixed-effects package, [lme4](https://cran.r-project.org/web/packages/lme4/index.html). To learn more about how brms compares to lme4, see Bürkner's [overview](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf).

The primary function in brms is `brm()`. Into this one function we will specify the data, the model, the likelihood function, the prior(s), and any technical settings such as the number of MCMC chains, iterations, and so forth.

```{r, warning = F, message = F, results = 'hide'}
library(brms)

fit1 <-
  brm(data = z15N50, 
      family = bernoulli(link = "identity"),
      y ~ 1,
      prior = set_prior("beta(2, 2)", class = "Intercept"),
      iter = 500 + 3334, warmup = 500, chains = 3)
```

### 8.2.3. Initialize chains.

In Stan, and in brms by extension, the initial values have default settings. On page 117 of the [Stan User’s Guide](http://mc-stan.org/users/documentation/index.html) (version 2.17.0) we read: "If there are no user-supplied initial values, the default initialization strategy is to initialize the unconstrained parameters directly with values drawn uniformly from the interval (−2, 2)." In general, I don’t recommend setting custom initial values in brms or Stan. Under the hood, Stan will transform the parameters to the unconstrained space in models where they are bounded. In our Bernoulli model, $\theta$ is bounded at 0 and 1. From page 118 of the Stan User’s Guide, we read

>For parameters bounded above and below, the initial value of 0 on the unconstrained scale corresponds to a value at the midpoint of the constraint interval. For probability parameters, bounded below by 0 and above by 1, the transform is the inverse logit, so that an initial unconstrained value of 0 corresponds to a constrained value of 0.5, -2 corresponds to 0.12 and 2 to 0.88. Bounds other than 0 and 1 are just scaled and translated.

If you want to play around with this, have at it.

### 8.2.4. Generate chains.

By default, brms will use 4 chains of 2000 iterations each. The type of MCMC brms uses is Hamiltonian Monte Carlo (HMC). You can learn more about HMC at the [Stan website](http://mc-stan.org), which includes the [user's guide](http://mc-stan.org/users/documentation/index.html) and a list of [tutorials](http://mc-stan.org/users/documentation/tutorials.html). McElreath also has a [nice intro lecture](https://www.youtube.com/watch?v=BWEtS3HuU5A&t=163s&frags=pl%2Cwn) on MCMC in general and HMC in particular.

Within each HMC chain, the first $n$ iterations are warmups. Within the Stan-HMC paradigm, [warmups are somewhat analogous to but not synonymous with burn-in iterations](http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/) as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. As such, the brms default settings yield 1000 post-warmup iterations for each of the 4 HMC chains. However, we specified `iter = 500 + 3334, warmup = 500, chains = 3`. Thus instead of defaults, we have 3 HMC chains. Each chain has 500 + 3334 = 3834 total iterations, of which 500 were discarded `warmup` iterations.

### 8.2.5. Examine chains.

The `brms::plot()` function returns a density and trace plot for each model parameter.

```{r, fig.width = 10, fig.height = 1.5}
plot(fit1)
```

If you want to display each chain as its own density, you can use the handy `mcmc_dens_overlay()` function from the [bayesplot package](https://cran.r-project.org/web/packages/bayesplot/index.html). But before we do so, we’ll need to export the posterior samples into a data frame, for which we’ll employ `posterior_samples()`.


```{r}
post <- posterior_samples(fit1, add_chain = T)
```

Note the `add_chain = T` argument, which will allow us to differentiate the draws by their chain of origin. But anyway, here are the overlaid densities. 

```{r, fig.width = 4, fig.height = 2, message = F, warning = F}
library(bayesplot)
mcmc_dens_overlay(post, pars = c("b_Intercept"))
```

With `bayesplot::mcmc_acf()`, we can look at the autocorrelation plots.

```{r, fig.width = 4, fig.height = 4}
mcmc_acf(post, pars = "b_Intercept", lags = 35)
```

As far as I can tell, brms and the other Stan interfaces for the R ecosystem don’t produce running plots for the Gelman-Rubin statistic like how Kruschke shows in the text. In brm and other R packages for Stan,  the Gelman-Rubin statistic is typically referred to as some variation of 'Rhat'. You can readily extract the Rhat values for a given parameter with the `bayesplot::rhat()` function.

```{r}
rhat(fit1)["b_Intercept"]
```

If you really wanted a running Rhat plot like in the lower left panel of Figure 8.3, you would employ the `coda::gelman.plot()` function. But you can’t just dump your `brm()` fit object into `coda::gelman.plot()`. It’s the wrong object type. However, brms offers the `as.mcmc()` function which will convert `brm()` objects for use in coda package functions.

```{r}
fit1_c <- as.mcmc(fit1)

fit1_c %>% glimpse()
```

With our freshly-converted `fit1_c` object in hand, we’re ready to plot.

```{r, fig.width = 4, fig.height = 3}
coda::gelman.plot(fit1_c[, "b_Intercept", ])
```

#### 8.2.5.1. ~~The `plotPost` function~~ How to plot your brms posterior distributions.

We’ll get into plotting in just a moment. But before we do, it makes sense to show how you might get a summary of the model. As far as I can tell, the `brms::print()` and `brms::summary()` functions return the same information. Since it requires less typing, here’s the `print()` output.

```{r}
print(fit1)
```

To summarize a posterior in terms of central tendency, brms defaults to the mean value (i.e., the value in the 'Estimate' column of the `print()` output). In many of the other convenience functions, you can also request the median instead. For example, we can use the `robust = T` argument to get the 'Estimate' in terms of the median. 

```{r}
posterior_summary(fit1, robust = T)
```

Across functions, the intervals default to 95%. With `print()` and `summary()` you can adjust the level with a `prob` argument.

```{r}
print(fit1, prob = .5)
```

But in many other brms convenience functions, you can use the `probs` argument to request specific percentile summaries.

```{r}
posterior_summary(fit1, probs = c(.025, .25, .75, .975))
```

Regardless of what `prob` or `probs` levels you use, brms functions always return percentile-based estimates. All this central tendency and interval talk will be important in a moment...

When plotting the posterior distribution of a parameter estimated with brms, you typically do so working with the results of an object returned by `posterior_samples()`. Recall we already saved those samples as `post`.

```{r}
head(post)
```

With `post` in hand, we can use ggplot2 to do the typical distributional plots, such as with `geom_histogram()`.

```{r, echo = F}
theme_set(theme_grey())
```

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
post %>% 
  ggplot(aes(x = b_Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank())
```

Since brms doesn’t have a convenient way to compute the posterior mode and since base R is no help, either, we’ll have to make our own function if we want the mode.

```{r}
estimate_mode <- function(x) {
  d <- density(x)
  d$x[which.max(d$y)]
  }
```

The [HDInterval package](https://cran.r-project.org/web/packages/HDInterval/index.html), of which Kruschke is one of the authors, offers a quick way to get HDIs. The [ggstance  package](https://github.com/lionel-/ggstance) allows one to use `stat_summaryh()` to use `HDInterval::hdi()` to mark off the HDIs as horizontal lines below a histogram or density.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
library(ggstance)
library(HDInterval)

my_breaks <-
  tibble(breaks = c(hdi(post$b_Intercept, credMass = .95)[1],
                    estimate_mode(post$b_Intercept),
                    hdi(post$b_Intercept, credMass = .95)[2])) %>% 
  mutate(labels = breaks %>% round(digits = 3))

post %>% 
  ggplot(aes(x = b_Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  stat_summaryh(aes(y = 0),
                fun.x = estimate_mode,
                fun.xmin = function(x){hdi(x, credMass = .95)[1]},
                fun.xmax = function(x){hdi(x, credMass = .95)[2]},
                size = 1, shape = 20) +
  scale_x_continuous(breaks = my_breaks$breaks,
                     labels = my_breaks$labels) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank())
```

`bayesplot::mcmc_areas()` offers a nice way to depict the posterior densities, along with their 50% and 95% ranges.

```{r, fig.width = 4, fig.height = 2.5, message = F}
mcmc_areas(
  post, 
  pars = c("b_Intercept"),
  prob = 0.5,
  prob_outer = 0.95,
  point_est = "mean"
) +
  scale_y_discrete(NULL, breaks = NULL) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank())
```

The next will take advantage of Matthew Kay's [tidybayes package](https://github.com/mjskay/tidybayes), which is currently only available via GitHub:

```{r, eval = F}
devtools::install_github("mjskay/tidybayes")
```

The tidybayes package gives us access to the handy `geom_halfeyeh()` function.

```{r, fig.width = 4, fig.height = 2.5, warning = F, message = F}
library(tidybayes)

post %>% 
  ggplot(aes(x = b_Intercept, y = 0)) +
  geom_halfeyeh(point_interval = mode_hdi,
                .prob = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank())
```

`geom_halfeyeh()` yields a density with a measure of the posterior’s central tendency in a dot and one or multiple interval bands as horizontal lines at the base of the density. Since we used the `point_interval = mode_hdi` argument, we asked for the mode to be our measure of central tendency and the highest posterior density intervals to be our intervals. With `.prob = c(.95, .5)`, we requested our HDIs be at both the 95% and 50% levels.

To be more congruent with Kruschke’s plotting sensibilities, we can combine `geom_histogram()` with `stat_pointintervalh()`. 

```{r, fig.width = 5, fig.height = 2.5, warning = F, message = F}
# This will allow us to make the breaks on the x-axis more meaningful
my_breaks <-
  tibble(breaks = c(estimate_mode(post$b_Intercept), 
                  hdi(post$b_Intercept, .prob = .95),
                  hdi(post$b_Intercept, .prob = .5))) %>% 
  mutate(labels = breaks %>% round(digits = 2))

post %>% 
  ggplot(aes(x = b_Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .prob = c(.95, .50)) +
  scale_x_continuous(breaks = my_breaks$breaks,
                     labels = my_breaks$labels) +
  scale_y_continuous(NULL, breaks = NULL,
                     expand = c(.1, 0)) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank())
```

With the `point_interval` argument within `tidybayes::stat_pointintervalh()`, we can request different combinations of measures of central tendency (i.e., mean, median, mode) and interval types (i.e., percentile-based and HDIs). Although all of these are legitimate ways to summarize a posterior, they can yield somewhat different results. For example, here we’ll contrast our mode + HDI summary with a median + percentile-based interval summary.

```{r, fig.width = 4, fig.height = 1.5}
post %>% 
  ggplot(aes(x = b_Intercept)) +
  stat_pointintervalh(aes(y = 1), point_interval = median_qi, .prob = c(.95, .5)) +
  stat_pointintervalh(aes(y = 2), point_interval = mode_hdi, .prob = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = 1:2,
                     labels = c("median_qi", "mode_hdi")) +
  coord_cartesian(ylim = 0:3) +
  labs(title = "Theta",
       x = expression(theta)) +
  theme(panel.grid = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(hjust = 0))
```

## 8.3. Simplified scripts for frequently used analyses

A lot has happened in R for Bayesian analysis since Kruschke wrote his text. In addition to our use of the tidyverse, the brms, bayesplot, and tidybayes packages offer an array of useful convenience functions. We can and occasionally will write our own. But really, the rich R ecosystem already has us pretty much covered.

## 8.4. Example: Difference of biases

Here are our new `z6N8z2N7` data.

```{r, message = F}
z6N8z2N7 <- read_csv("data.R/z6N8z2N7.csv")

glimpse(z6N8z2N7)
```

They look like this.

```{r, fig.width = 5, fig.height = 3}
z6N8z2N7 %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(x = y)) +
  geom_bar() +
  theme(panel.grid = element_blank()) +
  facet_wrap(~s)
```

Here we fit the model with `brm()`.

```{r, warning = F, message = F}
fit2 <-
  brm(data = z6N8z2N7, family = bernoulli("identity"),
      y ~ 0 + s,
      prior = c(set_prior("beta(2, 2)", class = "b", coef = "sReginald"),
                set_prior("beta(2, 2)", class = "b", coef = "sTony")),
      cores = 4, chains = 4,
      # This isn't always necessary, but it will let us use `prior_samples()` later
      sample_prior = T,
      control = list(adapt_delta = .999))
```

More typically, we’d parameterize the model as `y ~ 1 + s`. This form would yield an intercept and a slope. Behind the scenes, brms would treat the nominal `s` variable as an 0-1 coded dummy variable. One of the nominal levels would become the reverence category, depicted by the `Intercept`, and the difference between that and the other category would be the `s` slope. However, with our `y ~ 0 + s` syntax, we’ve suppressed the typical model intercept. The consequence is that each level of the nominal variable `s` gets its own intercept or [i] index, if you will. This is analogous to Kruschke’s `y[i] ∼ dbern(theta[s[i]])` code.

Also, notice our use of the `control = list(adapt_delta = .999)` argument. By default, `adapt_delta = .8`. Leaving it at its default for this model resulted in “divergent transitions after warmup” warnings, which urged me to increase “adapt_delta above 0.8.” Raising it to .999 worked. See the [brms user’s manual](https://cran.r-project.org/web/packages/brms/brms.pdf) for more on `adapt_delta`.

All that aside, here are the chains.

```{r, fig.width = 10, fig.height = 3}
plot(fit2)
```

The model `summary()` is as follows:

```{r}
summary(fit2)
```

The `brms::pairs()` function gets us the bulk of Figure 8.6.

```{r, fig.width = 4.5, fig.height = 4}
pairs(fit2,
      off_diag_args = list(size = 1/3, alpha = 1/3))
```

But to get at that difference score distribution, we’ll have extract the posterior iterations with `posterior_samples()`, make difference score with `mutate()`, and manually plot with ggplot2.

```{r}
post <- posterior_samples(fit2)

post <-
  post %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony = b_sTony) %>% 
  mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony)

head(post)
```

```{r, fig.width = 10, fig.height = 3, message = F}
gathered_post <-
  post %>% 
  select(starts_with("theta")) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("theta_Reginald", "theta_Tony", "theta_Reginald - theta_Tony"))) 
  
gathered_post %>% 
  ggplot(aes(x = value, group = key)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .prob = c(.95, .50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = "free_x")
```

Here’s a way to get the numeric summaries out of `post`.

```{r}
gathered_post %>% 
  group_by(key) %>% 
  mode_hdi()
```

In this context, the `mode_hdi()` summary yields:

* `key` (i.e., the name we used to denote the parameters)
* `value` (i.e., the mode)
* `conf.low` (i.e., the lower level of the 95% HDI)
* `conf.high` (i.e., the upper level...)
* `.prob` (i.e., what interval we used)

## 8.5. Sampling from the prior distribution in ~~JAGS~~ brms

The `sample_prior = T` argument in our `brm()` code allowed us to extract prior samples with the well-named `prior_samples()` function.

```{r}
prior <- prior_samples(fit2)

head(prior)
```

With that we can make our prior histograms as in Figure 8.7.

```{r, fig.width = 10, fig.height = 3}
gathered_prior <-
  prior %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony = b_sTony) %>% 
  mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) %>% 
  gather() %>% 
  mutate(key = factor(key, levels = c("theta_Reginald", "theta_Tony", "theta_Reginald - theta_Tony"))) 
  
gathered_prior %>% 
  ggplot(aes(x = value, group = key)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .prob = c(.95, .50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = "free_x")
```

Here's how to get the scatter plot.

```{r, fig.width = 3.5, fig.height = 3}
prior %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony = b_sTony) %>% 
  
  ggplot(aes(x = theta_Reginald, y = theta_Tony)) +
  geom_point(alpha = 1/4) +
  theme(panel.grid = element_blank())
```

Or you could always use a two-dimensional density plot with `stat_density_2d()`.

```{r, fig.width = 4.25, fig.height = 3}
prior %>% 
  rename(theta_Reginald = b_sReginald,
         theta_Tony = b_sTony) %>% 
  
  ggplot(aes(x = theta_Reginald, y = theta_Tony)) +
  stat_density_2d(aes(fill = stat(density)), 
                  geom = "raster", contour = F) +
  scale_fill_viridis_c() +
  labs(x = expression(theta[1]),
       y = expression(theta[2])) +
  theme(panel.grid = element_blank())
```

### 8.6.1. Defining new likelihood functions.

The brms package offers an array of likelihood functions, which you can browse [here](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html). However, you can also make your own likelihood functions. Bürkner explained the method in this [vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html). 

## 8.7. Faster sampling with parallel processing in ~~runjags~~ `brms::brm()`

We don't need to open another package to sample in parallel in brms. In fact, we've already been doing that. Take another look at the code use used for the last model, `fit2`.

```{r, eval = F}
fit2 <-
  brm(data = z6N8z2N7, family = binomial("identity"),
      y ~ 0 + s,
      prior = c(set_prior("beta(2, 2)", class = "b", coef = "sReginald"),
                set_prior("beta(2, 2)", class = "b", coef = "sTony")),
      cores = 4, chains = 4,
      sample_prior = T,
      control = list(adapt_delta = .99))
```

See the `cores = 4, chains = 4` arguments? With that bit of code, we told `brms::brm()` we wanted 4 chains, which we ran in parallel across 4 cores.

## References

Kruschke, J. K. (2015). *Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan.* Burlington, MA: Academic Press/Elsevier.

```{r}
sessionInfo()
```

```{r, message = F, warning = F, echo = F}
# Here we'll remove our objects
rm(z15N50, fit1, post, fit1_c, estimate_mode, my_breaks, z6N8z2N7, fit2, gathered_post, prior, gathered_prior)
```

